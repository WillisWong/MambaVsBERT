{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4dc1cd9e-675f-40f2-a2c0-a4ae17469f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 15:01:02.148833: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744729262.168810  179544 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744729262.174853  179544 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1744729262.190408  179544 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744729262.190432  179544 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744729262.190435  179544 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744729262.190437  179544 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-15 15:01:02.195928: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import psutil\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    AutoTokenizer,\n",
    "    BertForSequenceClassification,\n",
    "    BertConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    PreTrainedModel,\n",
    "    PretrainedConfig,\n",
    "    set_seed\n",
    ")\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from transformers import TrainingArguments, Trainer, TrainerCallback\n",
    "from mamba_ssm.models.mixer_seq_simple import MixerModel\n",
    "\n",
    "# 设置随机种子以确保可复现性\n",
    "set_seed(202504)\n",
    "\n",
    "# 确定设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 创建结果目录\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "os.makedirs(\"results/mamba\", exist_ok=True) \n",
    "os.makedirs(\"results/transformer\", exist_ok=True)\n",
    "os.makedirs(\"results/figures\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ff9f913-356c-4d93-9929-bca1fa706069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建Mamba配置类\n",
    "class MambaConfig(PretrainedConfig):\n",
    "    model_type = \"mamba\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size=50277,\n",
    "        hidden_size=768,\n",
    "        intermediate_size=3072,  # 添加中间层大小参数\n",
    "        state_size=16,\n",
    "        num_hidden_layers=12,\n",
    "        num_classes=2,\n",
    "        pad_token_id=0,\n",
    "        max_position_embeddings=2048,\n",
    "        **kwargs\n",
    "    ):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.intermediate_size = intermediate_size  # 保存中间层大小\n",
    "        self.state_size = state_size\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.num_classes = num_classes\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        super().__init__(pad_token_id=pad_token_id, **kwargs)\n",
    "\n",
    "# 创建基于Mamba的序列分类模型\n",
    "class MambaForSequenceClassification(PreTrainedModel):\n",
    "    config_class = MambaConfig\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        \n",
    "        # 初始化Mamba模型 - 添加d_intermediate参数\n",
    "        self.mamba = MixerModel(\n",
    "            d_model=config.hidden_size,\n",
    "            n_layer=config.num_hidden_layers,\n",
    "            d_intermediate=config.intermediate_size,  # 添加d_intermediate\n",
    "            vocab_size=config.vocab_size,\n",
    "            ssm_cfg={\"d_state\": config.state_size}\n",
    "        )\n",
    "        \n",
    "        # 添加分类头\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_classes)\n",
    "        \n",
    "        # 初始化权重\n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        # 初始化分类头\n",
    "        if self.classifier is not None:\n",
    "            self.classifier.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            if self.classifier.bias is not None:\n",
    "                self.classifier.bias.data.zero_()\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        # 确保输入类型正确\n",
    "        if not isinstance(input_ids, torch.Tensor):\n",
    "            input_ids = torch.tensor(input_ids, dtype=torch.long, device=self.device)\n",
    "            \n",
    "        # 通过Mamba模型获取序列输出\n",
    "        # 注意：根据Mamba的实际API调整参数\n",
    "        outputs = self.mamba(input_ids)\n",
    "        \n",
    "        # 使用最后一个token的输出作为整个序列的表示\n",
    "        sequence_output = outputs[:, -1]\n",
    "        \n",
    "        # 通过分类器获取分类结果\n",
    "        logits = self.classifier(sequence_output)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.config.num_classes), labels.view(-1))\n",
    "        \n",
    "        return {\"loss\": loss, \"logits\": logits} if loss is not None else {\"logits\": logits}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a84aff6-9c09-4713-902f-f714201a57f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 修改评估指标计算函数，以适应不同的返回值结构\n",
    "def compute_metrics_with_efficiency(eval_pred, model_type, start_time=None):\n",
    "    # 检查eval_pred的类型和结构\n",
    "    if hasattr(eval_pred, 'predictions') and hasattr(eval_pred, 'label_ids'):\n",
    "        # 如果是PredictionOutput类型对象\n",
    "        predictions = eval_pred.predictions\n",
    "        labels = eval_pred.label_ids\n",
    "    elif isinstance(eval_pred, tuple):\n",
    "        # 如果是元组，按位置解包\n",
    "        if len(eval_pred) >= 2:\n",
    "            predictions = eval_pred[0]\n",
    "            labels = eval_pred[1]\n",
    "        else:\n",
    "            raise ValueError(f\"Expected at least 2 elements in eval_pred tuple, got {len(eval_pred)}\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected eval_pred type: {type(eval_pred)}\")\n",
    "    \n",
    "    # 对predictions进行必要的后处理\n",
    "    if len(predictions.shape) > 1 and predictions.shape[1] > 1:\n",
    "        # 如果是logits，转换为类别预测\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    # 计算准确率\n",
    "    accuracy = np.mean(predictions == labels)\n",
    "    \n",
    "    metrics = {\"accuracy\": accuracy}\n",
    "    \n",
    "    # 如果是完整评估（传入start_time），则计算训练时间\n",
    "    if start_time is not None:\n",
    "        training_time = time.time() - start_time\n",
    "        metrics[\"training_time\"] = training_time\n",
    "    \n",
    "    # 记录当前内存使用情况\n",
    "    memory_usage = psutil.Process().memory_info().rss / 1024 ** 2  # MB\n",
    "    metrics[\"memory_usage\"] = memory_usage\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# 推理速度测试函数\n",
    "def test_inference_speed(model, tokenizer, sentences, device, max_length=128):\n",
    "    # 将输入移至设备\n",
    "    inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # 根据模型类型移除不需要的参数\n",
    "    if \"MambaForSequenceClassification\" in model.__class__.__name__:\n",
    "        # Mamba模型不使用token_type_ids\n",
    "        if 'token_type_ids' in inputs:\n",
    "            del inputs['token_type_ids']\n",
    "    \n",
    "    # 预热\n",
    "    for _ in range(5):\n",
    "        _ = model(**inputs)\n",
    "    \n",
    "    # 测速\n",
    "    start_time = time.time()\n",
    "    for _ in range(100):  # 多次测试取平均\n",
    "        _ = model(**inputs)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    avg_time = (end_time - start_time) / 100\n",
    "    return avg_time\n",
    "\n",
    "# 改进的长序列处理能力测试函数\n",
    "def test_long_sequence_performance(model_name, tokenizer, model_class, config, device, lengths=[128, 256, 512, 1024, 2048, 4096, 8192, 16384]):\n",
    "    results = {}\n",
    "    \n",
    "    for length in lengths:\n",
    "        print(f\"Testing sequence length: {length}\")\n",
    "        \n",
    "        # 创建一个新的适配这个长度的模型\n",
    "        if \"mamba\" in model_name.lower():\n",
    "            # Mamba模型配置\n",
    "            test_config = MambaConfig(**config.to_dict())\n",
    "            test_config.max_position_embeddings = length\n",
    "            model = model_class(test_config).to(device)\n",
    "        else:  # transformer\n",
    "            # 检查是否是预训练配置还是自定义配置\n",
    "            if hasattr(config, 'to_dict'):\n",
    "                test_config = BertConfig(**config.to_dict())\n",
    "                test_config.max_position_embeddings = length\n",
    "                model = model_class(test_config).to(device)\n",
    "            else:\n",
    "                # 如果是预训练模型配置，直接使用模型类创建\n",
    "                test_config = BertConfig(\n",
    "                    vocab_size=config.vocab_size,\n",
    "                    hidden_size=config.hidden_size,\n",
    "                    num_hidden_layers=config.num_hidden_layers,\n",
    "                    num_attention_heads=config.num_attention_heads,\n",
    "                    intermediate_size=config.intermediate_size,\n",
    "                    max_position_embeddings=length,  # 调整最大序列长度\n",
    "                    num_labels=config.num_labels\n",
    "                )\n",
    "                model = model_class(test_config).to(device)\n",
    "        \n",
    "        # 生成一个随机的测试序列\n",
    "        random_tokens = torch.randint(100, 5000, (1, length)).to(device)\n",
    "        attention_mask = torch.ones_like(random_tokens)\n",
    "        \n",
    "        # 预热\n",
    "        for _ in range(3):\n",
    "            if \"mamba\" in model_name.lower():\n",
    "                _ = model(random_tokens, attention_mask=attention_mask)\n",
    "            else:\n",
    "                _ = model(random_tokens, attention_mask=attention_mask)\n",
    "        \n",
    "        # 测速\n",
    "        torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "        start_time = time.time()\n",
    "        for _ in range(10):\n",
    "            if \"mamba\" in model_name.lower():\n",
    "                _ = model(random_tokens, attention_mask=attention_mask)\n",
    "            else:\n",
    "                _ = model(random_tokens, attention_mask=attention_mask)\n",
    "        torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "        end_time = time.time()\n",
    "        \n",
    "        # 记录结果\n",
    "        avg_time = (end_time - start_time) / 10\n",
    "        if torch.cuda.is_available():\n",
    "            memory_usage = torch.cuda.max_memory_allocated() / 1024**2  # MB\n",
    "        else:\n",
    "            memory_usage = psutil.Process().memory_info().rss / 1024**2  # MB\n",
    "        \n",
    "        results[length] = {\"time\": avg_time, \"memory\": memory_usage}\n",
    "        \n",
    "        # 清理内存\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 统一的数据处理函数\n",
    "def process_sst2_data(tokenizer, max_length=128, test_mode=False):\n",
    "    # 加载SST-2数据集\n",
    "    datasets = load_dataset(\"glue\", \"sst2\")\n",
    "    \n",
    "    # 测试模式下只使用一小部分数据\n",
    "    if test_mode:\n",
    "        # 只使用前100个样本\n",
    "        test_size = 100\n",
    "        datasets[\"train\"] = datasets[\"train\"].select(range(test_size))\n",
    "        datasets[\"validation\"] = datasets[\"validation\"].select(range(test_size))\n",
    "        print(f\"TEST MODE: Using only {test_size} samples for training and validation\")\n",
    "    \n",
    "    # 定义预处理函数\n",
    "    def preprocess_function(examples):\n",
    "        return tokenizer(\n",
    "            examples[\"sentence\"],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=max_length\n",
    "        )\n",
    "    \n",
    "    # 应用预处理\n",
    "    tokenized_datasets = datasets.map(preprocess_function, batched=True)\n",
    "    \n",
    "    # 设置数据格式\n",
    "    tokenized_datasets = tokenized_datasets.remove_columns([\"sentence\", \"idx\"])\n",
    "    tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "    tokenized_datasets.set_format(\"torch\")\n",
    "    \n",
    "    return tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e257f16-d996-41a0-8215-6a9bcfbaff44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 改进的Mamba模型训练与评估函数\n",
    "def train_evaluate_mamba(test_mode=False):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Running Mamba experiment {'(TEST MODE)' if test_mode else ''} (FROM SCRATCH)\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Mamba实验配置 - 参数量与Transformer匹配\n",
    "    if test_mode:\n",
    "        mamba_config = MambaConfig(\n",
    "            vocab_size=30522,\n",
    "            hidden_size=128,\n",
    "            intermediate_size=512,\n",
    "            num_hidden_layers=2,\n",
    "            state_size=8,\n",
    "            pad_token_id=0,\n",
    "            num_classes=2,\n",
    "        )\n",
    "    else:\n",
    "        # 调整后的配置，参数量约5400万\n",
    "        mamba_config = MambaConfig(\n",
    "            vocab_size=30522,\n",
    "            hidden_size=512,\n",
    "            intermediate_size=2048,\n",
    "            num_hidden_layers=8,\n",
    "            state_size=16,\n",
    "            pad_token_id=0,\n",
    "            num_classes=2,\n",
    "        )\n",
    "    \n",
    "    # 使用与Transformer相同的tokenizer\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\", local_files_only=True)\n",
    "        print(\"使用本地缓存的tokenizer\")\n",
    "    except:\n",
    "        print(\"创建基本tokenizer...\")\n",
    "        from transformers import BertTokenizer\n",
    "        vocab_file = os.path.join(\"vocab.txt\")\n",
    "        if not os.path.exists(vocab_file):\n",
    "            # 如果没有vocab文件，创建一个简单的\n",
    "            print(\"创建简单的词汇表...\")\n",
    "            basic_vocab = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "            for i in range(97, 123):  # a-z\n",
    "                basic_vocab.append(chr(i))\n",
    "            for word in [\"the\", \"a\", \"an\", \"and\", \"or\", \"but\", \"if\", \"then\", \"else\", \"this\", \"that\"]:\n",
    "                basic_vocab.append(word)\n",
    "            with open(vocab_file, \"w\") as f:\n",
    "                for word in basic_vocab:\n",
    "                    f.write(word + \"\\n\")\n",
    "        \n",
    "        tokenizer = BertTokenizer(vocab_file=vocab_file, do_lower_case=True)\n",
    "    \n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # 数据处理\n",
    "    tokenized_datasets = process_sst2_data(tokenizer, max_length=128, test_mode=test_mode)\n",
    "    \n",
    "    # 创建模型\n",
    "    model = MambaForSequenceClassification(mamba_config).to(device)\n",
    "    \n",
    "    # 打印模型参数量\n",
    "    param_count = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Mamba model parameters: {param_count:,}\")\n",
    "    \n",
    "    # 使用与Transformer相似的学习率\n",
    "    mamba_lr = 1e-4\n",
    "    \n",
    "    # 设置训练参数 - 匹配Transformer的训练条件\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"./results/mamba/{'test' if test_mode else 'full'}_scratch\",\n",
    "        learning_rate=mamba_lr,\n",
    "        per_device_train_batch_size=16 if test_mode else 32,\n",
    "        per_device_eval_batch_size=16 if test_mode else 64,\n",
    "        num_train_epochs=1 if test_mode else 6,  # 匹配Transformer的训练轮数\n",
    "        weight_decay=0.01,\n",
    "        save_strategy=\"steps\",  # 确保保存检查点\n",
    "        save_steps=250,         # 每250步保存一次\n",
    "        save_total_limit=2,     # 只保留最近的2个检查点\n",
    "        seed=202504,\n",
    "        eval_strategy=\"steps\",  # 定期评估\n",
    "        eval_steps=250,         # 每250步评估一次\n",
    "        logging_steps=250,      # 每250步记录一次\n",
    "        # 添加warm-up步骤\n",
    "        warmup_ratio=0.1,\n",
    "        # 使用线性学习率调度\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        # 添加梯度累积步骤\n",
    "        gradient_accumulation_steps=2,\n",
    "        # 添加梯度裁剪\n",
    "        max_grad_norm=1.0,\n",
    "        # 使用AdamW优化器\n",
    "        optim=\"adamw_torch\",\n",
    "        # 加载最佳模型进行评估\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"accuracy\",\n",
    "        greater_is_better=True,\n",
    "    )\n",
    "    \n",
    "    # 创建优化回调以监控Mamba的训练过程\n",
    "    class MambaTrainingCallback(TrainerCallback):\n",
    "        def __init__(self):\n",
    "            self.best_accuracy = 0.0\n",
    "            self.best_step = 0\n",
    "            self.loss_history = []\n",
    "            self.accuracy_history = []\n",
    "            \n",
    "        def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n",
    "            if metrics and \"eval_accuracy\" in metrics:\n",
    "                current_accuracy = metrics[\"eval_accuracy\"]\n",
    "                self.accuracy_history.append((state.global_step, current_accuracy))\n",
    "                \n",
    "                print(f\"Step {state.global_step}: Accuracy = {current_accuracy:.4f}\")\n",
    "                \n",
    "                if current_accuracy > self.best_accuracy:\n",
    "                    self.best_accuracy = current_accuracy\n",
    "                    self.best_step = state.global_step\n",
    "                    print(f\"New best accuracy: {self.best_accuracy:.4f} at step {self.best_step}\")\n",
    "                    \n",
    "                    # 手动保存最佳模型\n",
    "                    model = kwargs.get('model')\n",
    "                    if model is not None:\n",
    "                        best_model_dir = os.path.join(args.output_dir, \"best_model\")\n",
    "                        os.makedirs(best_model_dir, exist_ok=True)\n",
    "                        model.save_pretrained(best_model_dir)\n",
    "                        print(f\"已手动保存最佳模型到 {best_model_dir}\")\n",
    "                    \n",
    "        def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "            if logs and \"loss\" in logs:\n",
    "                self.loss_history.append((state.global_step, logs[\"loss\"]))\n",
    "                \n",
    "                # 每500步打印训练曲线\n",
    "                if state.global_step % 500 == 0 and len(self.loss_history) > 1:\n",
    "                    recent_losses = [l[1] for l in self.loss_history[-10:]]\n",
    "                    avg_loss = sum(recent_losses) / len(recent_losses)\n",
    "                    print(f\"Step {state.global_step}: Recent average loss = {avg_loss:.4f}\")\n",
    "                    \n",
    "        def on_train_end(self, args, state, control, **kwargs):\n",
    "            print(f\"Training completed. Best accuracy: {self.best_accuracy:.4f} at step {self.best_step}\")\n",
    "            \n",
    "            # 保存训练曲线\n",
    "            if len(self.loss_history) > 0 and len(self.accuracy_history) > 0:\n",
    "                steps, losses = zip(*self.loss_history)\n",
    "                acc_steps, accuracies = zip(*self.accuracy_history)\n",
    "                \n",
    "                plt.figure(figsize=(12, 5))\n",
    "                \n",
    "                plt.subplot(1, 2, 1)\n",
    "                plt.plot(steps, losses)\n",
    "                plt.title('Training Loss')\n",
    "                plt.xlabel('Step')\n",
    "                plt.ylabel('Loss')\n",
    "                \n",
    "                plt.subplot(1, 2, 2)\n",
    "                plt.plot(acc_steps, accuracies)\n",
    "                plt.title('Validation Accuracy')\n",
    "                plt.xlabel('Step')\n",
    "                plt.ylabel('Accuracy')\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                os.makedirs('results/mamba/figures', exist_ok=True)\n",
    "                plt.savefig('results/mamba/figures/training_curves.png')\n",
    "                plt.close()\n",
    "                \n",
    "                # 保存训练历史到CSV\n",
    "                import pandas as pd\n",
    "                history_df = pd.DataFrame({\n",
    "                    'step': steps,\n",
    "                    'loss': losses\n",
    "                })\n",
    "                history_df.to_csv('results/mamba/figures/loss_history.csv', index=False)\n",
    "                \n",
    "                accuracy_df = pd.DataFrame({\n",
    "                    'step': acc_steps,\n",
    "                    'accuracy': accuracies\n",
    "                })\n",
    "                accuracy_df.to_csv('results/mamba/figures/accuracy_history.csv', index=False)\n",
    "    \n",
    "    # 创建训练器\n",
    "    mamba_callback = MambaTrainingCallback()\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_datasets[\"train\"],\n",
    "        eval_dataset=tokenized_datasets[\"validation\"],\n",
    "        compute_metrics=lambda eval_pred: compute_metrics_with_efficiency(\n",
    "            eval_pred, \"mamba\", start_time=None\n",
    "        ),\n",
    "        callbacks=[mamba_callback],\n",
    "        processing_class=tokenizer,  # 传入tokenizer以便保存\n",
    "    )\n",
    "    \n",
    "    # 记录开始时间\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # 训练\n",
    "    trainer.train()\n",
    "    \n",
    "    # 加载最佳模型进行最终评估\n",
    "    best_model_dir = os.path.join(training_args.output_dir, \"best_model\")\n",
    "    if os.path.exists(best_model_dir):\n",
    "        print(f\"加载最佳模型进行最终评估...\")\n",
    "        model = MambaForSequenceClassification.from_pretrained(best_model_dir).to(device)\n",
    "        trainer.model = model\n",
    "    else:\n",
    "        print(\"未找到手动保存的最佳模型，使用Trainer保存的最佳模型或当前模型\")\n",
    "    \n",
    "    # 评估\n",
    "    eval_results = trainer.evaluate()\n",
    "    \n",
    "    # 计算最终指标\n",
    "    eval_pred = trainer.predict(tokenized_datasets[\"validation\"])\n",
    "    metrics = compute_metrics_with_efficiency(eval_pred, \"mamba\")\n",
    "    \n",
    "    final_metrics = {\n",
    "        \"accuracy\": metrics[\"accuracy\"],\n",
    "        \"training_time\": time.time() - start_time,\n",
    "        \"memory_usage\": psutil.Process().memory_info().rss / 1024 ** 2\n",
    "    }\n",
    "    \n",
    "    # 获取预测和标签进行详细评估\n",
    "    from sklearn.metrics import precision_recall_fscore_support, roc_curve, auc, confusion_matrix\n",
    "    \n",
    "    predictions = eval_pred.predictions\n",
    "    labels = eval_pred.label_ids\n",
    "    \n",
    "    # 获取预测类别和概率\n",
    "    preds = np.argmax(predictions, axis=1)\n",
    "    probabilities = torch.nn.functional.softmax(torch.tensor(predictions), dim=1).numpy()\n",
    "    \n",
    "    # 计算详细指标\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    confusion_mat = confusion_matrix(labels, preds)\n",
    "    fpr, tpr, _ = roc_curve(labels, probabilities[:, 1])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    # 添加到final_metrics\n",
    "    final_metrics[\"precision\"] = precision\n",
    "    final_metrics[\"recall\"] = recall\n",
    "    final_metrics[\"f1\"] = f1\n",
    "    final_metrics[\"roc_auc\"] = roc_auc\n",
    "    final_metrics[\"confusion_matrix\"] = confusion_mat\n",
    "    final_metrics[\"fpr\"] = fpr\n",
    "    final_metrics[\"tpr\"] = tpr\n",
    "    \n",
    "    # 绘制并保存ROC曲线\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Mamba ROC Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    os.makedirs('results/mamba/metrics', exist_ok=True)\n",
    "    plt.savefig('results/mamba/metrics/roc_curve.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # 绘制并保存混淆矩阵\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(confusion_mat, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title('Mamba Confusion Matrix')\n",
    "    plt.colorbar()\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "    # 添加数值标签\n",
    "    for i in range(confusion_mat.shape[0]):\n",
    "        for j in range(confusion_mat.shape[1]):\n",
    "            plt.text(j, i, str(confusion_mat[i, j]),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if confusion_mat[i, j] > confusion_mat.max() / 2 else \"black\")\n",
    "    \n",
    "    plt.savefig('results/mamba/metrics/confusion_matrix.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # 测试模式下，简化长序列测试\n",
    "    if test_mode:\n",
    "        # 只测试一个短长度序列\n",
    "        test_sentences = [\"This is a test sentence.\"]\n",
    "        print(\"Quick inference speed test...\")\n",
    "        inference_speed = test_inference_speed(\n",
    "            model, tokenizer, [test_sentences[0]], device, max_length=32\n",
    "        )\n",
    "        final_metrics[\"inference_speed\"] = inference_speed\n",
    "        \n",
    "        # 简化长序列测试，只测试一两个长度\n",
    "        print(\"Quick long sequence test...\")\n",
    "        long_seq_results = test_long_sequence_performance(\n",
    "            \"mamba\", tokenizer, MambaForSequenceClassification, mamba_config, device,\n",
    "            lengths=[32, 64] if test_mode else [128, 256, 512, 1024, 2048, 4096, 8192, 16384]\n",
    "        )\n",
    "    else:\n",
    "        # 完整测试\n",
    "        test_sentences = [\n",
    "            \"This movie was great!\", \n",
    "            \"I hated this film.\", \n",
    "            \"A wonderful experience that I would recommend to everyone.\",\n",
    "            \"The acting was mediocre at best, and the plot was predictable.\",\n",
    "            \"I've never been so bored watching a film in my entire life.\"\n",
    "        ]\n",
    "        \n",
    "        print(\"Testing inference speed...\")\n",
    "        inference_speed = test_inference_speed(\n",
    "            model, tokenizer, test_sentences, device\n",
    "        )\n",
    "        final_metrics[\"inference_speed\"] = inference_speed\n",
    "        \n",
    "        # 长序列测试\n",
    "        print(\"Testing long sequence performance...\")\n",
    "        long_seq_results = test_long_sequence_performance(\n",
    "            \"mamba\", tokenizer, MambaForSequenceClassification, mamba_config, device\n",
    "        )\n",
    "    \n",
    "    final_metrics[\"long_sequence_results\"] = long_seq_results\n",
    "    \n",
    "    # 保存详细评估指标到文件\n",
    "    with open('results/mamba/metrics/detailed_metrics.txt', 'w') as f:\n",
    "        f.write(f\"MAMBA DETAILED METRICS\\n\")\n",
    "        f.write(\"=\"*50 + \"\\n\\n\")\n",
    "        f.write(f\"Accuracy: {final_metrics['accuracy']:.4f}\\n\")\n",
    "        f.write(f\"Precision: {precision:.4f}\\n\")\n",
    "        f.write(f\"Recall: {recall:.4f}\\n\")\n",
    "        f.write(f\"F1 Score: {f1:.4f}\\n\")\n",
    "        f.write(f\"ROC AUC: {roc_auc:.4f}\\n\\n\")\n",
    "        \n",
    "        f.write(\"Training Information:\\n\")\n",
    "        f.write(f\"Training time: {final_metrics['training_time']:.2f} seconds\\n\")\n",
    "        f.write(f\"Inference speed: {final_metrics['inference_speed']:.5f} seconds/sample\\n\")\n",
    "        f.write(f\"Memory usage: {final_metrics['memory_usage']:.2f} MB\\n\\n\")\n",
    "        \n",
    "        f.write(\"Confusion Matrix:\\n\")\n",
    "        f.write(str(confusion_mat) + \"\\n\\n\")\n",
    "        \n",
    "        f.write(\"Long Sequence Performance:\\n\")\n",
    "        for length, results in long_seq_results.items():\n",
    "            f.write(f\"Length {length}: Time = {results['time']:.5f}s, Memory = {results['memory']:.2f} MB\\n\")\n",
    "    \n",
    "    # 保存Mamba结果摘要\n",
    "    with open(\"results/mamba/summary_scratch.txt\", \"w\") as f:\n",
    "        f.write(f\"MAMBA RESULTS (FROM SCRATCH) {'(TEST MODE)' if test_mode else ''}\\\\n\")\n",
    "        f.write(\"=\"*50 + \"\\\\n\")\n",
    "        f.write(f\"Accuracy: {final_metrics['accuracy']:.4f}\\\\n\")\n",
    "        f.write(f\"Precision: {precision:.4f}\\\\n\")\n",
    "        f.write(f\"Recall: {recall:.4f}\\\\n\")\n",
    "        f.write(f\"F1 Score: {f1:.4f}\\\\n\")\n",
    "        f.write(f\"ROC AUC: {roc_auc:.4f}\\\\n\\\\n\")\n",
    "        f.write(f\"Training time: {final_metrics['training_time']:.2f} seconds\\\\n\")\n",
    "        f.write(f\"Inference speed: {final_metrics['inference_speed']:.5f} seconds/sample\\\\n\")\n",
    "        f.write(f\"Memory usage: {final_metrics['memory_usage']:.2f} MB\\\\n\\\\n\")\n",
    "        \n",
    "        f.write(\"LONG SEQUENCE PERFORMANCE\\\\n\")\n",
    "        f.write(\"=\"*50 + \"\\\\n\")\n",
    "        for length, results in long_seq_results.items():\n",
    "            f.write(f\"Length {length}: Time = {results['time']:.5f}s, Memory = {results['memory']:.2f} MB\\\\n\")\n",
    "    \n",
    "    return final_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78838b55-7130-4b81-9167-3be70c124450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 改进的Transformer训练函数\n",
    "def train_evaluate_transformer(test_mode=False):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Running Transformer experiment {'(TEST MODE)' if test_mode else ''} (FROM SCRATCH)\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # 使用本地tokenizer\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\", local_files_only=True)\n",
    "        print(\"使用本地缓存的tokenizer\")\n",
    "    except:\n",
    "        print(\"创建基本tokenizer...\")\n",
    "        from transformers import BertTokenizer\n",
    "        vocab_file = os.path.join(\"vocab.txt\")\n",
    "        if not os.path.exists(vocab_file):\n",
    "            # 如果没有vocab文件，创建一个简单的\n",
    "            print(\"创建简单的词汇表...\")\n",
    "            basic_vocab = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "            for i in range(97, 123):  # a-z\n",
    "                basic_vocab.append(chr(i))\n",
    "            for word in [\"the\", \"a\", \"an\", \"and\", \"or\", \"but\", \"if\", \"then\", \"else\", \"this\", \"that\"]:\n",
    "                basic_vocab.append(word)\n",
    "            with open(vocab_file, \"w\") as f:\n",
    "                for word in basic_vocab:\n",
    "                    f.write(word + \"\\n\")\n",
    "        \n",
    "        tokenizer = BertTokenizer(vocab_file=vocab_file, do_lower_case=True)\n",
    "    \n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # 数据处理\n",
    "    tokenized_datasets = process_sst2_data(tokenizer, max_length=128, test_mode=test_mode)\n",
    "    \n",
    "    # 创建模型 - 使用优化配置\n",
    "    if test_mode:\n",
    "        # 测试模式下使用较小的模型配置\n",
    "        transformer_config = BertConfig(\n",
    "            vocab_size=30522,\n",
    "            hidden_size=128,\n",
    "            num_hidden_layers=2,\n",
    "            num_attention_heads=2,\n",
    "            intermediate_size=512,\n",
    "            hidden_act=\"gelu\",\n",
    "            hidden_dropout_prob=0.1,\n",
    "            attention_probs_dropout_prob=0.1,\n",
    "            max_position_embeddings=512,\n",
    "            type_vocab_size=2,\n",
    "            initializer_range=0.08,\n",
    "            layer_norm_eps=1e-12,\n",
    "            pad_token_id=0,\n",
    "            num_labels=2,\n",
    "        )\n",
    "    else:\n",
    "        # 从头创建优化的BERT模型\n",
    "        print(\"从头创建优化的BERT模型...\")\n",
    "        transformer_config = BertConfig(\n",
    "            vocab_size=30522,\n",
    "            hidden_size=768,\n",
    "            num_hidden_layers=6,  # 6层\n",
    "            num_attention_heads=12,\n",
    "            intermediate_size=3072,\n",
    "            hidden_act=\"gelu\",\n",
    "            hidden_dropout_prob=0.1,\n",
    "            attention_probs_dropout_prob=0.1,\n",
    "            max_position_embeddings=512,\n",
    "            type_vocab_size=2,\n",
    "            initializer_range=0.08,  # 增加初始化范围\n",
    "            layer_norm_eps=1e-12,\n",
    "            pad_token_id=0,\n",
    "            num_labels=2,\n",
    "        )\n",
    "    \n",
    "    model = BertForSequenceClassification(transformer_config).to(device)\n",
    "    \n",
    "    # 打印模型参数量\n",
    "    param_count = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Transformer model parameters: {param_count:,}\")\n",
    "    \n",
    "    # 设置训练参数\n",
    "    transformer_lr = 1e-4  # 调整学习率\n",
    "    \n",
    "    # 设置训练参数 - 调整以优化从头训练性能\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"./results/transformer/{'test' if test_mode else 'full'}_scratch\",\n",
    "        learning_rate=transformer_lr,\n",
    "        per_device_train_batch_size=16 if test_mode else 32,\n",
    "        per_device_eval_batch_size=16 if test_mode else 64,\n",
    "        num_train_epochs=1 if test_mode else 6,\n",
    "        weight_decay=0.01,\n",
    "        save_strategy=\"steps\",  # 确保保存检查点\n",
    "        save_steps=250,         # 每250步保存一次\n",
    "        save_total_limit=2,     # 只保留最近的2个检查点\n",
    "        seed=202504,\n",
    "        eval_strategy=\"steps\",  # 定期评估\n",
    "        eval_steps=250,         # 每250步评估一次\n",
    "        logging_steps=250,      # 每250步记录一次\n",
    "        # 使用比例预热，更加灵活\n",
    "        warmup_ratio=0.1,\n",
    "        # 使用线性学习率调度\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        # 增加梯度累积步骤，帮助稳定训练\n",
    "        gradient_accumulation_steps=2,\n",
    "        # 添加梯度裁剪，避免梯度爆炸\n",
    "        max_grad_norm=1.0,\n",
    "        # 使用AdamW优化器\n",
    "        optim=\"adamw_torch\",\n",
    "        # 加载最佳模型进行评估\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"accuracy\",\n",
    "        greater_is_better=True,\n",
    "    )\n",
    "    \n",
    "    # 创建一个自定义回调来监控训练过程\n",
    "    class TransformerTrainingCallback(TrainerCallback):\n",
    "        def __init__(self):\n",
    "            self.best_accuracy = 0.0\n",
    "            self.best_step = 0\n",
    "            self.loss_history = []\n",
    "            self.accuracy_history = []\n",
    "            \n",
    "        def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n",
    "            if metrics and \"eval_accuracy\" in metrics:\n",
    "                current_accuracy = metrics[\"eval_accuracy\"]\n",
    "                self.accuracy_history.append((state.global_step, current_accuracy))\n",
    "                \n",
    "                print(f\"Step {state.global_step}: Accuracy = {current_accuracy:.4f}\")\n",
    "                \n",
    "                if current_accuracy > self.best_accuracy:\n",
    "                    self.best_accuracy = current_accuracy\n",
    "                    self.best_step = state.global_step\n",
    "                    print(f\"New best accuracy: {self.best_accuracy:.4f} at step {self.best_step}\")\n",
    "                    \n",
    "                    # 手动保存最佳模型\n",
    "                    model = kwargs.get('model')\n",
    "                    if model is not None:\n",
    "                        best_model_dir = os.path.join(args.output_dir, \"best_model\")\n",
    "                        os.makedirs(best_model_dir, exist_ok=True)\n",
    "                        model.save_pretrained(best_model_dir)\n",
    "                        print(f\"已手动保存最佳模型到 {best_model_dir}\")\n",
    "                    \n",
    "        def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "            if logs and \"loss\" in logs:\n",
    "                self.loss_history.append((state.global_step, logs[\"loss\"]))\n",
    "                \n",
    "                # 每500步打印训练曲线\n",
    "                if state.global_step % 500 == 0 and len(self.loss_history) > 1:\n",
    "                    recent_losses = [l[1] for l in self.loss_history[-10:]]\n",
    "                    avg_loss = sum(recent_losses) / len(recent_losses)\n",
    "                    print(f\"Step {state.global_step}: Recent average loss = {avg_loss:.4f}\")\n",
    "                    \n",
    "        def on_train_end(self, args, state, control, **kwargs):\n",
    "            print(f\"Training completed. Best accuracy: {self.best_accuracy:.4f} at step {self.best_step}\")\n",
    "            \n",
    "            # 保存训练曲线\n",
    "            if len(self.loss_history) > 0 and len(self.accuracy_history) > 0:\n",
    "                steps, losses = zip(*self.loss_history)\n",
    "                acc_steps, accuracies = zip(*self.accuracy_history)\n",
    "                \n",
    "                plt.figure(figsize=(12, 5))\n",
    "                \n",
    "                plt.subplot(1, 2, 1)\n",
    "                plt.plot(steps, losses)\n",
    "                plt.title('Training Loss')\n",
    "                plt.xlabel('Step')\n",
    "                plt.ylabel('Loss')\n",
    "                \n",
    "                plt.subplot(1, 2, 2)\n",
    "                plt.plot(acc_steps, accuracies)\n",
    "                plt.title('Validation Accuracy')\n",
    "                plt.xlabel('Step')\n",
    "                plt.ylabel('Accuracy')\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                os.makedirs('results/transformer/figures', exist_ok=True)\n",
    "                plt.savefig('results/transformer/figures/training_curves.png')\n",
    "                plt.close()\n",
    "                \n",
    "                # 保存训练历史到CSV\n",
    "                import pandas as pd\n",
    "                history_df = pd.DataFrame({\n",
    "                    'step': steps,\n",
    "                    'loss': losses\n",
    "                })\n",
    "                history_df.to_csv('results/transformer/figures/loss_history.csv', index=False)\n",
    "                \n",
    "                accuracy_df = pd.DataFrame({\n",
    "                    'step': acc_steps,\n",
    "                    'accuracy': accuracies\n",
    "                })\n",
    "                accuracy_df.to_csv('results/transformer/figures/accuracy_history.csv', index=False)\n",
    "    \n",
    "    # 创建训练器\n",
    "    transformer_callback = TransformerTrainingCallback()\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_datasets[\"train\"],\n",
    "        eval_dataset=tokenized_datasets[\"validation\"],\n",
    "        compute_metrics=lambda eval_pred: compute_metrics_with_efficiency(\n",
    "            eval_pred, \"transformer\", start_time=None\n",
    "        ),\n",
    "        callbacks=[transformer_callback],\n",
    "        processing_class=tokenizer,  # 传入tokenizer以便保存\n",
    "    )\n",
    "    \n",
    "    # 记录开始时间\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # 训练\n",
    "    trainer.train()\n",
    "    \n",
    "    # 加载最佳模型进行最终评估\n",
    "    best_model_dir = os.path.join(training_args.output_dir, \"best_model\")\n",
    "    if os.path.exists(best_model_dir):\n",
    "        print(f\"加载最佳模型进行最终评估...\")\n",
    "        model = BertForSequenceClassification.from_pretrained(best_model_dir).to(device)\n",
    "        trainer.model = model\n",
    "    else:\n",
    "        print(\"未找到手动保存的最佳模型，使用Trainer保存的最佳模型或当前模型\")\n",
    "    \n",
    "    # 评估\n",
    "    eval_results = trainer.evaluate()\n",
    "    \n",
    "    # 计算最终指标\n",
    "    eval_pred = trainer.predict(tokenized_datasets[\"validation\"])\n",
    "    metrics = compute_metrics_with_efficiency(eval_pred, \"transformer\")\n",
    "    \n",
    "    final_metrics = {\n",
    "        \"accuracy\": metrics[\"accuracy\"],\n",
    "        \"training_time\": time.time() - start_time,\n",
    "        \"memory_usage\": psutil.Process().memory_info().rss / 1024 ** 2\n",
    "    }\n",
    "    \n",
    "    # 获取预测和标签进行详细评估\n",
    "    from sklearn.metrics import precision_recall_fscore_support, roc_curve, auc, confusion_matrix\n",
    "    \n",
    "    predictions = eval_pred.predictions\n",
    "    labels = eval_pred.label_ids\n",
    "    \n",
    "    # 获取预测类别和概率\n",
    "    preds = np.argmax(predictions, axis=1)\n",
    "    probabilities = torch.nn.functional.softmax(torch.tensor(predictions), dim=1).numpy()\n",
    "    \n",
    "    # 计算详细指标\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    confusion_mat = confusion_matrix(labels, preds)\n",
    "    fpr, tpr, _ = roc_curve(labels, probabilities[:, 1])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    # 添加到final_metrics\n",
    "    final_metrics[\"precision\"] = precision\n",
    "    final_metrics[\"recall\"] = recall\n",
    "    final_metrics[\"f1\"] = f1\n",
    "    final_metrics[\"roc_auc\"] = roc_auc\n",
    "    final_metrics[\"confusion_matrix\"] = confusion_mat\n",
    "    final_metrics[\"fpr\"] = fpr\n",
    "    final_metrics[\"tpr\"] = tpr\n",
    "    \n",
    "    # 绘制并保存ROC曲线\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Transformer ROC Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    os.makedirs('results/transformer/metrics', exist_ok=True)\n",
    "    plt.savefig('results/transformer/metrics/roc_curve.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # 绘制并保存混淆矩阵\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(confusion_mat, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title('Transformer Confusion Matrix')\n",
    "    plt.colorbar()\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "    # 添加数值标签\n",
    "    for i in range(confusion_mat.shape[0]):\n",
    "        for j in range(confusion_mat.shape[1]):\n",
    "            plt.text(j, i, str(confusion_mat[i, j]),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if confusion_mat[i, j] > confusion_mat.max() / 2 else \"black\")\n",
    "    \n",
    "    plt.savefig('results/transformer/metrics/confusion_matrix.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # 测试模式下，简化长序列测试\n",
    "    if test_mode:\n",
    "        # 只测试一个短长度序列\n",
    "        test_sentences = [\"This is a test sentence.\"]\n",
    "        print(\"Quick inference speed test...\")\n",
    "        inference_speed = test_inference_speed(\n",
    "            model, tokenizer, [test_sentences[0]], device, max_length=32\n",
    "        )\n",
    "        final_metrics[\"inference_speed\"] = inference_speed\n",
    "        \n",
    "        # 简化长序列测试，只测试一两个长度\n",
    "        print(\"Quick long sequence test...\")\n",
    "        long_seq_results = test_long_sequence_performance(\n",
    "            \"transformer\", tokenizer, BertForSequenceClassification, transformer_config, device,\n",
    "            lengths=[32, 64] if test_mode else [128, 256, 512, 1024, 2048, 4096, 8192, 16384]\n",
    "        )\n",
    "    else:\n",
    "        # 完整测试\n",
    "        test_sentences = [\n",
    "            \"This movie was great!\", \n",
    "            \"I hated this film.\", \n",
    "            \"A wonderful experience that I would recommend to everyone.\",\n",
    "            \"The acting was mediocre at best, and the plot was predictable.\",\n",
    "            \"I've never been so bored watching a film in my entire life.\"\n",
    "        ]\n",
    "        \n",
    "        print(\"Testing inference speed...\")\n",
    "        inference_speed = test_inference_speed(\n",
    "            model, tokenizer, test_sentences, device\n",
    "        )\n",
    "        final_metrics[\"inference_speed\"] = inference_speed\n",
    "        \n",
    "        # 长序列测试\n",
    "        print(\"Testing long sequence performance...\")\n",
    "        long_seq_results = test_long_sequence_performance(\n",
    "            \"transformer\", tokenizer, BertForSequenceClassification, transformer_config, device\n",
    "        )\n",
    "    \n",
    "    final_metrics[\"long_sequence_results\"] = long_seq_results\n",
    "    \n",
    "    # 保存详细评估指标到文件\n",
    "    with open('results/transformer/metrics/detailed_metrics.txt', 'w') as f:\n",
    "        f.write(f\"TRANSFORMER DETAILED METRICS\\n\")\n",
    "        f.write(\"=\"*50 + \"\\n\\n\")\n",
    "        f.write(f\"Accuracy: {final_metrics['accuracy']:.4f}\\n\")\n",
    "        f.write(f\"Precision: {precision:.4f}\\n\")\n",
    "        f.write(f\"Recall: {recall:.4f}\\n\")\n",
    "        f.write(f\"F1 Score: {f1:.4f}\\n\")\n",
    "        f.write(f\"ROC AUC: {roc_auc:.4f}\\n\\n\")\n",
    "        \n",
    "        f.write(\"Training Information:\\n\")\n",
    "        f.write(f\"Training time: {final_metrics['training_time']:.2f} seconds\\n\")\n",
    "        f.write(f\"Inference speed: {final_metrics['inference_speed']:.5f} seconds/sample\\n\")\n",
    "        f.write(f\"Memory usage: {final_metrics['memory_usage']:.2f} MB\\n\\n\")\n",
    "        \n",
    "        f.write(\"Confusion Matrix:\\n\")\n",
    "        f.write(str(confusion_mat) + \"\\n\\n\")\n",
    "        \n",
    "        f.write(\"Long Sequence Performance:\\n\")\n",
    "        for length, results in long_seq_results.items():\n",
    "            f.write(f\"Length {length}: Time = {results['time']:.5f}s, Memory = {results['memory']:.2f} MB\\n\")\n",
    "    \n",
    "    # 保存Transformer结果摘要\n",
    "    with open(\"results/transformer/summary_scratch.txt\", \"w\") as f:\n",
    "        f.write(f\"TRANSFORMER RESULTS (FROM SCRATCH) {'(TEST MODE)' if test_mode else ''}\\\\n\")\n",
    "        f.write(\"=\"*50 + \"\\\\n\")\n",
    "        f.write(f\"Accuracy: {final_metrics['accuracy']:.4f}\\\\n\")\n",
    "        f.write(f\"Precision: {precision:.4f}\\\\n\")\n",
    "        f.write(f\"Recall: {recall:.4f}\\\\n\")\n",
    "        f.write(f\"F1 Score: {f1:.4f}\\\\n\")\n",
    "        f.write(f\"ROC AUC: {roc_auc:.4f}\\\\n\\\\n\")\n",
    "        f.write(f\"Training time: {final_metrics['training_time']:.2f} seconds\\\\n\")\n",
    "        f.write(f\"Inference speed: {final_metrics['inference_speed']:.5f} seconds/sample\\\\n\")\n",
    "        f.write(f\"Memory usage: {final_metrics['memory_usage']:.2f} MB\\\\n\\\\n\")\n",
    "        \n",
    "        f.write(\"LONG SEQUENCE PERFORMANCE\\\\n\")\n",
    "        f.write(\"=\"*50 + \"\\\\n\")\n",
    "        for length, results in long_seq_results.items():\n",
    "            f.write(f\"Length {length}: Time = {results['time']:.5f}s, Memory = {results['memory']:.2f} MB\\\\n\")\n",
    "    \n",
    "    return final_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da54739b-bbba-4875-b22b-e1088442ca3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 结果可视化函数\n",
    "def visualize_results(mamba_results, transformer_results):\n",
    "    # 创建结果目录\n",
    "    os.makedirs(\"results/figures\", exist_ok=True)\n",
    "    \n",
    "    # 准确率和速度对比\n",
    "    metrics = [\"accuracy\", \"training_time\", \"inference_speed\", \"memory_usage\"]\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, metric in enumerate(metrics):\n",
    "        data = {\n",
    "            \"Mamba\": mamba_results[metric],\n",
    "            \"Transformer\": transformer_results[metric]\n",
    "        }\n",
    "        ax = axes[i]\n",
    "        bars = ax.bar(data.keys(), data.values())\n",
    "        ax.set_title(f\"{metric.replace('_', ' ').title()}\")\n",
    "        \n",
    "        # 添加数值标签\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f\"{height:.4f}\",\n",
    "                    ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"results/figures/basic_metrics_comparison.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    # 长序列性能对比\n",
    "    lengths = list(mamba_results[\"long_sequence_results\"].keys())\n",
    "    \n",
    "    # 时间对比\n",
    "    mamba_times = [mamba_results[\"long_sequence_results\"][l][\"time\"] for l in lengths]\n",
    "    transformer_times = [transformer_results[\"long_sequence_results\"][l][\"time\"] for l in lengths]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(lengths, mamba_times, 'o-', label='Mamba')\n",
    "    plt.plot(lengths, transformer_times, 's-', label='Transformer')\n",
    "    plt.xlabel('Sequence Length')\n",
    "    plt.ylabel('Inference Time (s)')\n",
    "    plt.title('Inference Time vs Sequence Length')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(\"results/figures/sequence_length_time.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    # 内存对比\n",
    "    mamba_memory = [mamba_results[\"long_sequence_results\"][l][\"memory\"] for l in lengths]\n",
    "    transformer_memory = [transformer_results[\"long_sequence_results\"][l][\"memory\"] for l in lengths]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(lengths, mamba_memory, 'o-', label='Mamba')\n",
    "    plt.plot(lengths, transformer_memory, 's-', label='Transformer')\n",
    "    plt.xlabel('Sequence Length')\n",
    "    plt.ylabel('Memory Usage (MB)')\n",
    "    plt.title('Memory Usage vs Sequence Length')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(\"results/figures/sequence_length_memory.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    # 创建性能比率图 - 这将显示Mamba相对于Transformer的优势\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    time_ratios = [transformer_times[i]/mamba_times[i] for i in range(len(lengths))]\n",
    "    memory_ratios = [transformer_memory[i]/mamba_memory[i] for i in range(len(lengths))]\n",
    "    \n",
    "    plt.plot(lengths, time_ratios, 'o-', label='Time Ratio (Transformer/Mamba)')\n",
    "    plt.plot(lengths, memory_ratios, 's-', label='Memory Ratio (Transformer/Mamba)')\n",
    "    plt.axhline(y=1, color='r', linestyle='--')\n",
    "    plt.xlabel('Sequence Length')\n",
    "    plt.ylabel('Ratio (Transformer/Mamba)')\n",
    "    plt.title('Performance Ratio vs Sequence Length\\n(Higher means Mamba is more efficient)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(\"results/figures/performance_ratio.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    # 保存数值结果到CSV\n",
    "    results_df = pd.DataFrame({\n",
    "        'Metric': metrics,\n",
    "        'Mamba': [mamba_results[m] for m in metrics],\n",
    "        'Transformer': [transformer_results[m] for m in metrics],\n",
    "        'Ratio (Transformer/Mamba)': [transformer_results[m]/mamba_results[m] if mamba_results[m] > 0 else 0 for m in metrics]\n",
    "    })\n",
    "    results_df.to_csv(\"results/basic_metrics.csv\", index=False)\n",
    "    \n",
    "    # 长序列结果\n",
    "    long_seq_df = pd.DataFrame({\n",
    "        'Length': lengths,\n",
    "        'Mamba_Time': mamba_times,\n",
    "        'Transformer_Time': transformer_times,\n",
    "        'Time_Ratio': time_ratios,\n",
    "        'Mamba_Memory': mamba_memory,\n",
    "        'Transformer_Memory': transformer_memory,\n",
    "        'Memory_Ratio': memory_ratios\n",
    "    })\n",
    "    long_seq_df.to_csv(\"results/long_sequence_metrics.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef869c2e-8a01-4cbf-bc2c-b21f4d45d1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 详细的模型比较函数\n",
    "def compare_detailed_results(mamba_results, transformer_results):\n",
    "    \"\"\"\n",
    "    比较两个模型的详细结果，生成综合报告和可视化\n",
    "    \n",
    "    参数:\n",
    "        mamba_results: Mamba模型的结果字典\n",
    "        transformer_results: Transformer模型的结果字典\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import pandas as pd\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "    \n",
    "    # 创建输出目录\n",
    "    os.makedirs(\"results/comparison\", exist_ok=True)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"DETAILED MODEL COMPARISON\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # 1. 基本性能指标\n",
    "    metrics = [\"accuracy\", \"precision\", \"recall\", \"f1\", \"roc_auc\"]\n",
    "    metric_names = [\"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\", \"ROC AUC\"]\n",
    "    \n",
    "    print(\"\\nCLASSIFICATION PERFORMANCE\")\n",
    "    print(\"-\"*30)\n",
    "    \n",
    "    # 创建表格数据\n",
    "    comparison_data = {\n",
    "        \"Metric\": metric_names,\n",
    "        \"Mamba\": [mamba_results.get(m, 0) for m in metrics],\n",
    "        \"Transformer\": [transformer_results.get(m, 0) for m in metrics],\n",
    "    }\n",
    "    \n",
    "    # 计算差异和比率\n",
    "    comparison_data[\"Difference\"] = [\n",
    "        comparison_data[\"Mamba\"][i] - comparison_data[\"Transformer\"][i] \n",
    "        for i in range(len(metrics))\n",
    "    ]\n",
    "    \n",
    "    comparison_data[\"Ratio (M/T)\"] = [\n",
    "        comparison_data[\"Mamba\"][i] / comparison_data[\"Transformer\"][i] if comparison_data[\"Transformer\"][i] > 0 else 0\n",
    "        for i in range(len(metrics))\n",
    "    ]\n",
    "    \n",
    "    # 打印比较结果\n",
    "    for i, metric in enumerate(metric_names):\n",
    "        print(f\"{metric}:\")\n",
    "        print(f\"  Mamba: {comparison_data['Mamba'][i]:.4f}\")\n",
    "        print(f\"  Transformer: {comparison_data['Transformer'][i]:.4f}\")\n",
    "        print(f\"  Difference: {comparison_data['Difference'][i]:.4f}\")\n",
    "        print(f\"  Ratio (M/T): {comparison_data['Ratio (M/T)'][i]:.4f}\")\n",
    "        print()\n",
    "    \n",
    "    # 2. 资源和效率指标\n",
    "    efficiency_metrics = [\"training_time\", \"inference_speed\", \"memory_usage\"]\n",
    "    efficiency_names = [\"Training Time (s)\", \"Inference Speed (s/sample)\", \"Memory Usage (MB)\"]\n",
    "    \n",
    "    print(\"\\nEFFICIENCY METRICS\")\n",
    "    print(\"-\"*30)\n",
    "    \n",
    "    efficiency_data = {\n",
    "        \"Metric\": efficiency_names,\n",
    "        \"Mamba\": [mamba_results.get(m, 0) for m in efficiency_metrics],\n",
    "        \"Transformer\": [transformer_results.get(m, 0) for m in efficiency_metrics],\n",
    "    }\n",
    "    \n",
    "    # 计算比率 - 注意对于效率指标，较小值更好，所以计算T/M\n",
    "    efficiency_data[\"Ratio (T/M)\"] = [\n",
    "        efficiency_data[\"Transformer\"][i] / efficiency_data[\"Mamba\"][i] if efficiency_data[\"Mamba\"][i] > 0 else 0\n",
    "        for i in range(len(efficiency_metrics))\n",
    "    ]\n",
    "    \n",
    "    # 打印效率比较结果\n",
    "    for i, metric in enumerate(efficiency_names):\n",
    "        print(f\"{metric}:\")\n",
    "        print(f\"  Mamba: {efficiency_data['Mamba'][i]:.4f}\")\n",
    "        print(f\"  Transformer: {efficiency_data['Transformer'][i]:.4f}\")\n",
    "        print(f\"  Ratio (T/M): {efficiency_data['Ratio (T/M)'][i]:.4f}\")\n",
    "        print(f\"  {'Mamba' if efficiency_data['Ratio (T/M)'][i] > 1 else 'Transformer'} is more efficient\")\n",
    "        print()\n",
    "    \n",
    "    # 3. 长序列性能比较\n",
    "    if \"long_sequence_results\" in mamba_results and \"long_sequence_results\" in transformer_results:\n",
    "        print(\"\\nLONG SEQUENCE PERFORMANCE\")\n",
    "        print(\"-\"*30)\n",
    "        \n",
    "        # 获取长度数据\n",
    "        lengths = list(mamba_results[\"long_sequence_results\"].keys())\n",
    "        \n",
    "        # 收集时间和内存数据\n",
    "        mamba_times = []\n",
    "        transformer_times = []\n",
    "        mamba_memory = []\n",
    "        transformer_memory = []\n",
    "        time_ratios = []\n",
    "        memory_ratios = []\n",
    "        \n",
    "        for length in lengths:\n",
    "            mamba_time = mamba_results[\"long_sequence_results\"][length][\"time\"]\n",
    "            transformer_time = transformer_results[\"long_sequence_results\"][length][\"time\"]\n",
    "            mamba_mem = mamba_results[\"long_sequence_results\"][length][\"memory\"]\n",
    "            transformer_mem = transformer_results[\"long_sequence_results\"][length][\"memory\"]\n",
    "            \n",
    "            time_ratio = transformer_time / mamba_time if mamba_time > 0 else 0\n",
    "            memory_ratio = transformer_mem / mamba_mem if mamba_mem > 0 else 0\n",
    "            \n",
    "            mamba_times.append(mamba_time)\n",
    "            transformer_times.append(transformer_time)\n",
    "            mamba_memory.append(mamba_mem)\n",
    "            transformer_memory.append(transformer_mem)\n",
    "            time_ratios.append(time_ratio)\n",
    "            memory_ratios.append(memory_ratio)\n",
    "            \n",
    "            print(f\"Sequence Length {length}:\")\n",
    "            print(f\"  Time (s): Mamba = {mamba_time:.5f}, Transformer = {transformer_time:.5f}, Ratio (T/M) = {time_ratio:.2f}x\")\n",
    "            print(f\"  Memory (MB): Mamba = {mamba_mem:.2f}, Transformer = {transformer_mem:.2f}, Ratio (T/M) = {memory_ratio:.2f}x\")\n",
    "            print(f\"  {'Mamba' if time_ratio > 1 else 'Transformer'} is faster\")\n",
    "            print(f\"  {'Mamba' if memory_ratio > 1 else 'Transformer'} is more memory efficient\")\n",
    "            print()\n",
    "    \n",
    "    # 创建可视化\n",
    "    \n",
    "    # 1. 分类性能条形图\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    x = np.arange(len(metric_names))\n",
    "    width = 0.35\n",
    "    \n",
    "    plt.bar(x - width/2, comparison_data[\"Mamba\"], width, label='Mamba', color='blue', alpha=0.7)\n",
    "    plt.bar(x + width/2, comparison_data[\"Transformer\"], width, label='Transformer', color='red', alpha=0.7)\n",
    "    \n",
    "    plt.xlabel('Metrics')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Classification Performance Comparison')\n",
    "    plt.xticks(x, metric_names)\n",
    "    plt.legend()\n",
    "    \n",
    "    # 添加数值标签\n",
    "    for i, v in enumerate(comparison_data[\"Mamba\"]):\n",
    "        plt.text(i - width/2, v + 0.02, f\"{v:.3f}\", ha='center')\n",
    "    \n",
    "    for i, v in enumerate(comparison_data[\"Transformer\"]):\n",
    "        plt.text(i + width/2, v + 0.02, f\"{v:.3f}\", ha='center')\n",
    "    \n",
    "    plt.ylim(0, max(max(comparison_data[\"Mamba\"]), max(comparison_data[\"Transformer\"])) * 1.2)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"results/comparison/classification_metrics.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. 效率指标条形图\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    x = np.arange(len(efficiency_names))\n",
    "    \n",
    "    # 归一化效率指标以便在同一图表上显示\n",
    "    max_values = [max(efficiency_data[\"Mamba\"][i], efficiency_data[\"Transformer\"][i]) for i in range(len(efficiency_metrics))]\n",
    "    norm_mamba = [efficiency_data[\"Mamba\"][i] / max_values[i] for i in range(len(efficiency_metrics))]\n",
    "    norm_transformer = [efficiency_data[\"Transformer\"][i] / max_values[i] for i in range(len(efficiency_metrics))]\n",
    "    \n",
    "    plt.bar(x - width/2, norm_mamba, width, label='Mamba', color='blue', alpha=0.7)\n",
    "    plt.bar(x + width/2, norm_transformer, width, label='Transformer', color='red', alpha=0.7)\n",
    "    \n",
    "    plt.xlabel('Metrics')\n",
    "    plt.ylabel('Normalized Value (lower is better)')\n",
    "    plt.title('Efficiency Metrics Comparison (Normalized)')\n",
    "    plt.xticks(x, efficiency_names)\n",
    "    plt.legend()\n",
    "    \n",
    "    # 添加原始数值标签\n",
    "    for i, v in enumerate(efficiency_data[\"Mamba\"]):\n",
    "        plt.text(i - width/2, norm_mamba[i] + 0.05, f\"{v:.2f}\", ha='center')\n",
    "    \n",
    "    for i, v in enumerate(efficiency_data[\"Transformer\"]):\n",
    "        plt.text(i + width/2, norm_transformer[i] + 0.05, f\"{v:.2f}\", ha='center')\n",
    "    \n",
    "    plt.ylim(0, 1.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"results/comparison/efficiency_metrics.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    # 3. 长序列性能图\n",
    "    if \"long_sequence_results\" in mamba_results and \"long_sequence_results\" in transformer_results:\n",
    "        # 时间对比\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(lengths, mamba_times, 'o-', label='Mamba', color='blue')\n",
    "        plt.plot(lengths, transformer_times, 's-', label='Transformer', color='red')\n",
    "        plt.xlabel('Sequence Length')\n",
    "        plt.ylabel('Inference Time (s)')\n",
    "        plt.title('Inference Time vs Sequence Length')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.savefig(\"results/comparison/sequence_length_time.png\")\n",
    "        plt.close()\n",
    "        \n",
    "        # 内存对比\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(lengths, mamba_memory, 'o-', label='Mamba', color='blue')\n",
    "        plt.plot(lengths, transformer_memory, 's-', label='Transformer', color='red')\n",
    "        plt.xlabel('Sequence Length')\n",
    "        plt.ylabel('Memory Usage (MB)')\n",
    "        plt.title('Memory Usage vs Sequence Length')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.savefig(\"results/comparison/sequence_length_memory.png\")\n",
    "        plt.close()\n",
    "        \n",
    "        # 性能比率\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(lengths, time_ratios, 'o-', label='Time Ratio (T/M)', color='blue')\n",
    "        plt.plot(lengths, memory_ratios, 's-', label='Memory Ratio (T/M)', color='red')\n",
    "        plt.axhline(y=1, color='gray', linestyle='--')\n",
    "        plt.xlabel('Sequence Length')\n",
    "        plt.ylabel('Ratio (Transformer/Mamba)')\n",
    "        plt.title('Performance Ratio vs Sequence Length\\n(Higher means Mamba is more efficient)')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.savefig(\"results/comparison/performance_ratio.png\")\n",
    "        plt.close()\n",
    "    \n",
    "    # 4. ROC曲线比较\n",
    "    if all(k in mamba_results for k in [\"fpr\", \"tpr\", \"roc_auc\"]) and all(k in transformer_results for k in [\"fpr\", \"tpr\", \"roc_auc\"]):\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.plot(mamba_results[\"fpr\"], mamba_results[\"tpr\"], \n",
    "                 color='blue', lw=2, \n",
    "                 label=f'Mamba ROC (AUC = {mamba_results[\"roc_auc\"]:.4f})')\n",
    "        plt.plot(transformer_results[\"fpr\"], transformer_results[\"tpr\"], \n",
    "                 color='red', lw=2, \n",
    "                 label=f'Transformer ROC (AUC = {transformer_results[\"roc_auc\"]:.4f})')\n",
    "        plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('ROC Curves Comparison')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.savefig(\"results/comparison/roc_comparison.png\")\n",
    "        plt.close()\n",
    "    \n",
    "    # 创建混淆矩阵比较（如果可用）\n",
    "    if \"confusion_matrix\" in mamba_results and \"confusion_matrix\" in transformer_results:\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "        \n",
    "        # Mamba混淆矩阵\n",
    "        im1 = ax1.imshow(mamba_results[\"confusion_matrix\"], interpolation='nearest', cmap=plt.cm.Blues)\n",
    "        ax1.set_title('Mamba Confusion Matrix')\n",
    "        ax1.set_xlabel('Predicted Label')\n",
    "        ax1.set_ylabel('True Label')\n",
    "        \n",
    "        # 添加文本\n",
    "        for i in range(mamba_results[\"confusion_matrix\"].shape[0]):\n",
    "            for j in range(mamba_results[\"confusion_matrix\"].shape[1]):\n",
    "                ax1.text(j, i, format(mamba_results[\"confusion_matrix\"][i, j], 'd'),\n",
    "                       ha=\"center\", va=\"center\",\n",
    "                       color=\"white\" if mamba_results[\"confusion_matrix\"][i, j] > mamba_results[\"confusion_matrix\"].max() / 2 else \"black\")\n",
    "        \n",
    "        # Transformer混淆矩阵\n",
    "        im2 = ax2.imshow(transformer_results[\"confusion_matrix\"], interpolation='nearest', cmap=plt.cm.Reds)\n",
    "        ax2.set_title('Transformer Confusion Matrix')\n",
    "        ax2.set_xlabel('Predicted Label')\n",
    "        ax2.set_ylabel('True Label')\n",
    "        \n",
    "        # 添加文本\n",
    "        for i in range(transformer_results[\"confusion_matrix\"].shape[0]):\n",
    "            for j in range(transformer_results[\"confusion_matrix\"].shape[1]):\n",
    "                ax2.text(j, i, format(transformer_results[\"confusion_matrix\"][i, j], 'd'),\n",
    "                       ha=\"center\", va=\"center\",\n",
    "                       color=\"white\" if transformer_results[\"confusion_matrix\"][i, j] > transformer_results[\"confusion_matrix\"].max() / 2 else \"black\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"results/comparison/confusion_matrices.png\")\n",
    "        plt.close()\n",
    "    \n",
    "    # 保存比较结果到CSV\n",
    "    pd.DataFrame(comparison_data).to_csv(\"results/comparison/classification_metrics.csv\", index=False)\n",
    "    pd.DataFrame(efficiency_data).to_csv(\"results/comparison/efficiency_metrics.csv\", index=False)\n",
    "    \n",
    "    # 如果有长序列数据，保存到CSV\n",
    "    if \"long_sequence_results\" in mamba_results and \"long_sequence_results\" in transformer_results:\n",
    "        long_seq_df = pd.DataFrame({\n",
    "            'Length': lengths,\n",
    "            'Mamba_Time': mamba_times,\n",
    "            'Transformer_Time': transformer_times,\n",
    "            'Time_Ratio': time_ratios,\n",
    "            'Mamba_Memory': mamba_memory,\n",
    "            'Transformer_Memory': transformer_memory,\n",
    "            'Memory_Ratio': memory_ratios\n",
    "        })\n",
    "        long_seq_df.to_csv(\"results/comparison/long_sequence_metrics.csv\", index=False)\n",
    "    \n",
    "    # 保存详细比较报告到文本文件\n",
    "    with open(\"results/comparison/detailed_comparison.txt\", \"w\") as f:\n",
    "        f.write(\"DETAILED MODEL COMPARISON REPORT\\n\")\n",
    "        f.write(\"=\"*50 + \"\\n\\n\")\n",
    "        \n",
    "        # 1. 分类性能\n",
    "        f.write(\"CLASSIFICATION PERFORMANCE\\n\")\n",
    "        f.write(\"-\"*30 + \"\\n\\n\")\n",
    "        \n",
    "        for i, metric in enumerate(metric_names):\n",
    "            f.write(f\"{metric}:\\n\")\n",
    "            f.write(f\"  Mamba: {comparison_data['Mamba'][i]:.4f}\\n\")\n",
    "            f.write(f\"  Transformer: {comparison_data['Transformer'][i]:.4f}\\n\")\n",
    "            f.write(f\"  Difference: {comparison_data['Difference'][i]:.4f}\\n\")\n",
    "            f.write(f\"  Ratio (M/T): {comparison_data['Ratio (M/T)'][i]:.4f}\\n\\n\")\n",
    "        \n",
    "        # 2. 效率指标\n",
    "        f.write(\"\\nEFFICIENCY METRICS\\n\")\n",
    "        f.write(\"-\"*30 + \"\\n\\n\")\n",
    "        \n",
    "        for i, metric in enumerate(efficiency_names):\n",
    "            f.write(f\"{metric}:\\n\")\n",
    "            f.write(f\"  Mamba: {efficiency_data['Mamba'][i]:.4f}\\n\")\n",
    "            f.write(f\"  Transformer: {efficiency_data['Transformer'][i]:.4f}\\n\")\n",
    "            f.write(f\"  Ratio (T/M): {efficiency_data['Ratio (T/M)'][i]:.4f}\\n\")\n",
    "            f.write(f\"  {'Mamba' if efficiency_data['Ratio (T/M)'][i] > 1 else 'Transformer'} is more efficient\\n\\n\")\n",
    "        \n",
    "        # 3. 长序列性能\n",
    "        if \"long_sequence_results\" in mamba_results and \"long_sequence_results\" in transformer_results:\n",
    "            f.write(\"\\nLONG SEQUENCE PERFORMANCE\\n\")\n",
    "            f.write(\"-\"*30 + \"\\n\\n\")\n",
    "            \n",
    "            for i, length in enumerate(lengths):\n",
    "                f.write(f\"Sequence Length {length}:\\n\")\n",
    "                f.write(f\"  Time (s): Mamba = {mamba_times[i]:.5f}, Transformer = {transformer_times[i]:.5f}, Ratio (T/M) = {time_ratios[i]:.2f}x\\n\")\n",
    "                f.write(f\"  Memory (MB): Mamba = {mamba_memory[i]:.2f}, Transformer = {transformer_memory[i]:.2f}, Ratio (T/M) = {memory_ratios[i]:.2f}x\\n\")\n",
    "                f.write(f\"  {'Mamba' if time_ratios[i] > 1 else 'Transformer'} is faster\\n\")\n",
    "                f.write(f\"  {'Mamba' if memory_ratios[i] > 1 else 'Transformer'} is more memory efficient\\n\\n\")\n",
    "        \n",
    "        # 4. 总结\n",
    "        f.write(\"\\nSUMMARY\\n\")\n",
    "        f.write(\"-\"*30 + \"\\n\\n\")\n",
    "        \n",
    "        # 分类性能总结\n",
    "        avg_accuracy_diff = comparison_data[\"Difference\"][0]\n",
    "        if avg_accuracy_diff > 0.02:\n",
    "            f.write(f\"Mamba outperforms Transformer in classification accuracy by {avg_accuracy_diff:.4f}\\n\")\n",
    "        elif avg_accuracy_diff < -0.02:\n",
    "            f.write(f\"Transformer outperforms Mamba in classification accuracy by {-avg_accuracy_diff:.4f}\\n\")\n",
    "        else:\n",
    "            f.write(f\"Mamba and Transformer have similar classification accuracy (difference: {avg_accuracy_diff:.4f})\\n\")\n",
    "        \n",
    "        # 效率总结\n",
    "        avg_time_ratio = efficiency_data[\"Ratio (T/M)\"][0]\n",
    "        avg_memory_ratio = efficiency_data[\"Ratio (T/M)\"][2]\n",
    "        \n",
    "        if avg_time_ratio > 1.1:\n",
    "            f.write(f\"Mamba is {avg_time_ratio:.2f}x faster in training than Transformer\\n\")\n",
    "        elif avg_time_ratio < 0.9:\n",
    "            f.write(f\"Transformer is {1/avg_time_ratio:.2f}x faster in training than Mamba\\n\")\n",
    "        else:\n",
    "            f.write(f\"Mamba and Transformer have similar training speed\\n\")\n",
    "        \n",
    "        if avg_memory_ratio > 1.1:\n",
    "            f.write(f\"Mamba uses {1/avg_memory_ratio:.2f}x less memory than Transformer\\n\")\n",
    "        elif avg_memory_ratio < 0.9:\n",
    "            f.write(f\"Transformer uses {avg_memory_ratio:.2f}x less memory than Mamba\\n\")\n",
    "        else:\n",
    "            f.write(f\"Mamba and Transformer have similar memory usage\\n\")\n",
    "        \n",
    "        # 长序列总结\n",
    "        if \"long_sequence_results\" in mamba_results and \"long_sequence_results\" in transformer_results:\n",
    "            avg_long_time_ratio = sum(time_ratios) / len(time_ratios)\n",
    "            avg_long_memory_ratio = sum(memory_ratios) / len(memory_ratios)\n",
    "            \n",
    "            if avg_long_time_ratio > 1.1:\n",
    "                f.write(f\"Mamba is on average {avg_long_time_ratio:.2f}x faster for long sequences than Transformer\\n\")\n",
    "            elif avg_long_time_ratio < 0.9:\n",
    "                f.write(f\"Transformer is on average {1/avg_long_time_ratio:.2f}x faster for long sequences than Mamba\\n\")\n",
    "            else:\n",
    "                f.write(f\"Mamba and Transformer have similar speed for long sequences\\n\")\n",
    "            \n",
    "            if avg_long_memory_ratio > 1.1:\n",
    "                f.write(f\"Mamba uses on average {1/avg_long_memory_ratio:.2f}x less memory for long sequences than Transformer\\n\")\n",
    "            elif avg_long_memory_ratio < 0.9:\n",
    "                f.write(f\"Transformer uses on average {avg_long_memory_ratio:.2f}x less memory for long sequences than Mamba\\n\")\n",
    "            else:\n",
    "                f.write(f\"Mamba and Transformer have similar memory usage for long sequences\\n\")\n",
    "        \n",
    "        # 最终结论\n",
    "        f.write(\"\\nFINAL CONCLUSION:\\n\")\n",
    "        if comparison_data[\"Difference\"][0] > 0 and avg_time_ratio > 1:\n",
    "            f.write(\"Mamba appears to be both more accurate and more efficient than Transformer for this task.\\n\")\n",
    "        elif comparison_data[\"Difference\"][0] < 0 and avg_time_ratio < 1:\n",
    "            f.write(\"Transformer appears to be both more accurate and more efficient than Mamba for this task.\\n\")\n",
    "        elif comparison_data[\"Difference\"][0] > 0 and avg_time_ratio < 1:\n",
    "            f.write(\"Mamba appears to be more accurate but less efficient than Transformer for this task.\\n\")\n",
    "        elif comparison_data[\"Difference\"][0] < 0 and avg_time_ratio > 1:\n",
    "            f.write(\"Transformer appears to be more accurate but less efficient than Mamba for this task.\\n\")\n",
    "        else:\n",
    "            f.write(\"Both models show comparable performance with different trade-offs.\\n\")\n",
    "    \n",
    "    print(\"\\nDetailed comparison completed. Results saved to 'results/comparison/' directory.\")\n",
    "    \n",
    "    return {\n",
    "        \"classification_metrics\": comparison_data,\n",
    "        \"efficiency_metrics\": efficiency_data,\n",
    "        \"long_sequence_metrics\": {\"lengths\": lengths, \"time_ratios\": time_ratios, \"memory_ratios\": memory_ratios} \n",
    "        if \"long_sequence_results\" in mamba_results and \"long_sequence_results\" in transformer_results else None\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "440c80bb-967f-402d-90d1-8b0a9f2481c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 运行完整实验的函数\n",
    "def run_complete_experiment(test_mode=False):\n",
    "    \"\"\"\n",
    "    运行完整的模型比较实验\n",
    "    \n",
    "    参数:\n",
    "        test_mode: 是否运行测试模式(默认: False)\n",
    "    \n",
    "    返回:\n",
    "        dict: 包含实验结果的字典\n",
    "    \"\"\"\n",
    "    import time\n",
    "    import os\n",
    "    \n",
    "    # 记录开始时间\n",
    "    experiment_start_time = time.time()\n",
    "    \n",
    "    print(f\"\\n{'='*30} STARTING EXPERIMENT {'(TEST MODE)' if test_mode else ''} {'='*30}\")\n",
    "    \n",
    "    # 创建结果目录\n",
    "    os.makedirs(\"results\", exist_ok=True)\n",
    "    os.makedirs(\"results/mamba\", exist_ok=True)\n",
    "    os.makedirs(\"results/transformer\", exist_ok=True)\n",
    "    os.makedirs(\"results/comparison\", exist_ok=True)\n",
    "    \n",
    "    # 训练Mamba模型\n",
    "    print(\"\\n[1/3] Training and evaluating Mamba model...\")\n",
    "    try:\n",
    "        mamba_results = train_evaluate_mamba(test_mode)\n",
    "        print(\"Mamba training and evaluation completed successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in Mamba experiment: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        mamba_results = None\n",
    "    \n",
    "    # 训练Transformer模型\n",
    "    print(\"\\n[2/3] Training and evaluating Transformer model...\")\n",
    "    try:\n",
    "        transformer_results = train_evaluate_transformer(test_mode)\n",
    "        print(\"Transformer training and evaluation completed successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in Transformer experiment: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        transformer_results = None\n",
    "    \n",
    "    # 比较结果\n",
    "    if mamba_results is not None and transformer_results is not None:\n",
    "        print(\"\\n[3/3] Comparing model results...\")\n",
    "        try:\n",
    "            comparison_results = compare_detailed_results(mamba_results, transformer_results)\n",
    "            print(\"Model comparison completed successfully!\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error in model comparison: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            comparison_results = None\n",
    "    else:\n",
    "        print(\"\\n[3/3] Skipping comparison as one or both models failed.\")\n",
    "        comparison_results = None\n",
    "    \n",
    "    # 计算总实验时间\n",
    "    total_time = time.time() - experiment_start_time\n",
    "    hours, remainder = divmod(total_time, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    \n",
    "    print(f\"\\n{'='*30} EXPERIMENT COMPLETED {'='*30}\")\n",
    "    print(f\"Total experiment time: {int(hours)}h {int(minutes)}m {seconds:.2f}s\")\n",
    "    \n",
    "    # 保存总结果\n",
    "    with open(\"results/experiment_summary.txt\", \"w\") as f:\n",
    "        f.write(\"MAMBA VS TRANSFORMER EXPERIMENT SUMMARY\\n\")\n",
    "        f.write(\"=\"*50 + \"\\n\\n\")\n",
    "        \n",
    "        f.write(f\"Experiment mode: {'TEST' if test_mode else 'FULL'}\\n\")\n",
    "        f.write(f\"Total experiment time: {int(hours)}h {int(minutes)}m {seconds:.2f}s\\n\\n\")\n",
    "        \n",
    "        if mamba_results is not None:\n",
    "            f.write(\"Mamba model: SUCCESS\\n\")\n",
    "            f.write(f\"  - Accuracy: {mamba_results.get('accuracy', 'N/A'):.4f}\\n\")\n",
    "            f.write(f\"  - F1 Score: {mamba_results.get('f1', 'N/A'):.4f}\\n\")\n",
    "            f.write(f\"  - Training time: {mamba_results.get('training_time', 'N/A'):.2f}s\\n\\n\")\n",
    "        else:\n",
    "            f.write(\"Mamba model: FAILED\\n\\n\")\n",
    "        \n",
    "        if transformer_results is not None:\n",
    "            f.write(\"Transformer model: SUCCESS\\n\")\n",
    "            f.write(f\"  - Accuracy: {transformer_results.get('accuracy', 'N/A'):.4f}\\n\")\n",
    "            f.write(f\"  - F1 Score: {transformer_results.get('f1', 'N/A'):.4f}\\n\")\n",
    "            f.write(f\"  - Training time: {transformer_results.get('training_time', 'N/A'):.2f}s\\n\\n\")\n",
    "        else:\n",
    "            f.write(\"Transformer model: FAILED\\n\\n\")\n",
    "        \n",
    "        if comparison_results is not None:\n",
    "            f.write(\"Models comparison: SUCCESS\\n\")\n",
    "            f.write(\"  - See 'results/comparison/' directory for detailed reports and visualizations\\n\\n\")\n",
    "        else:\n",
    "            f.write(\"Models comparison: FAILED or SKIPPED\\n\\n\")\n",
    "        \n",
    "        f.write(\"\\nResults directories:\\n\")\n",
    "        f.write(\"  - Mamba results: ./results/mamba/\\n\")\n",
    "        f.write(\"  - Transformer results: ./results/transformer/\\n\")\n",
    "        f.write(\"  - Comparison results: ./results/comparison/\\n\")\n",
    "    \n",
    "    # 返回结果\n",
    "    return {\n",
    "        \"mamba_results\": mamba_results,\n",
    "        \"transformer_results\": transformer_results,\n",
    "        \"comparison_results\": comparison_results,\n",
    "        \"total_time\": total_time\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f9a4e25-3b39-4a1c-865c-fdd277e012e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================== STARTING EXPERIMENT  ==============================\n",
      "\n",
      "[1/3] Training and evaluating Mamba model...\n",
      "\n",
      "==================================================\n",
      "Running Mamba experiment  (FROM SCRATCH)\n",
      "==================================================\n",
      "使用本地缓存的tokenizer\n",
      "Mamba model parameters: 54,369,282\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6312' max='6312' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6312/6312 33:55, Epoch 5/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Memory Usage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.714600</td>\n",
       "      <td>0.708354</td>\n",
       "      <td>0.490826</td>\n",
       "      <td>2115.070312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.680200</td>\n",
       "      <td>0.606504</td>\n",
       "      <td>0.649083</td>\n",
       "      <td>2165.082031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.464900</td>\n",
       "      <td>0.470075</td>\n",
       "      <td>0.783257</td>\n",
       "      <td>2157.386719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.346300</td>\n",
       "      <td>0.431795</td>\n",
       "      <td>0.793578</td>\n",
       "      <td>2157.519531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.281100</td>\n",
       "      <td>0.502349</td>\n",
       "      <td>0.779817</td>\n",
       "      <td>2159.078125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.244600</td>\n",
       "      <td>0.461170</td>\n",
       "      <td>0.788991</td>\n",
       "      <td>2159.347656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.236700</td>\n",
       "      <td>0.473358</td>\n",
       "      <td>0.809633</td>\n",
       "      <td>2159.394531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.217100</td>\n",
       "      <td>0.507201</td>\n",
       "      <td>0.806193</td>\n",
       "      <td>2159.441406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.173200</td>\n",
       "      <td>0.456599</td>\n",
       "      <td>0.830275</td>\n",
       "      <td>2160.222656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.143400</td>\n",
       "      <td>0.549011</td>\n",
       "      <td>0.795872</td>\n",
       "      <td>2160.300781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.143900</td>\n",
       "      <td>0.500041</td>\n",
       "      <td>0.817661</td>\n",
       "      <td>2160.304688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.141900</td>\n",
       "      <td>0.520943</td>\n",
       "      <td>0.815367</td>\n",
       "      <td>2160.304688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>0.121400</td>\n",
       "      <td>0.542663</td>\n",
       "      <td>0.822248</td>\n",
       "      <td>2160.304688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.083100</td>\n",
       "      <td>0.564178</td>\n",
       "      <td>0.819954</td>\n",
       "      <td>2160.304688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>0.088700</td>\n",
       "      <td>0.576347</td>\n",
       "      <td>0.802752</td>\n",
       "      <td>2160.308594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.087800</td>\n",
       "      <td>0.545522</td>\n",
       "      <td>0.817661</td>\n",
       "      <td>2160.308594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4250</td>\n",
       "      <td>0.083400</td>\n",
       "      <td>0.621499</td>\n",
       "      <td>0.822248</td>\n",
       "      <td>2160.308594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.048000</td>\n",
       "      <td>0.686867</td>\n",
       "      <td>0.795872</td>\n",
       "      <td>2160.308594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4750</td>\n",
       "      <td>0.055600</td>\n",
       "      <td>0.582365</td>\n",
       "      <td>0.818807</td>\n",
       "      <td>2160.308594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.054100</td>\n",
       "      <td>0.629174</td>\n",
       "      <td>0.813073</td>\n",
       "      <td>2160.308594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5250</td>\n",
       "      <td>0.050500</td>\n",
       "      <td>0.642882</td>\n",
       "      <td>0.816514</td>\n",
       "      <td>2160.308594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.029600</td>\n",
       "      <td>0.717384</td>\n",
       "      <td>0.818807</td>\n",
       "      <td>2160.308594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5750</td>\n",
       "      <td>0.027500</td>\n",
       "      <td>0.754514</td>\n",
       "      <td>0.800459</td>\n",
       "      <td>2160.308594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.026300</td>\n",
       "      <td>0.724208</td>\n",
       "      <td>0.818807</td>\n",
       "      <td>2160.308594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6250</td>\n",
       "      <td>0.025100</td>\n",
       "      <td>0.750295</td>\n",
       "      <td>0.807339</td>\n",
       "      <td>2160.308594</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 250: Accuracy = 0.4908\n",
      "New best accuracy: 0.4908 at step 250\n",
      "已手动保存最佳模型到 ./results/mamba/full_scratch/best_model\n",
      "Step 500: Recent average loss = 0.6974\n",
      "Step 500: Accuracy = 0.6491\n",
      "New best accuracy: 0.6491 at step 500\n",
      "已手动保存最佳模型到 ./results/mamba/full_scratch/best_model\n",
      "Step 750: Accuracy = 0.7833\n",
      "New best accuracy: 0.7833 at step 750\n",
      "已手动保存最佳模型到 ./results/mamba/full_scratch/best_model\n",
      "Step 1000: Recent average loss = 0.5515\n",
      "Step 1000: Accuracy = 0.7936\n",
      "New best accuracy: 0.7936 at step 1000\n",
      "已手动保存最佳模型到 ./results/mamba/full_scratch/best_model\n",
      "Step 1250: Accuracy = 0.7798\n",
      "Step 1500: Recent average loss = 0.4553\n",
      "Step 1500: Accuracy = 0.7890\n",
      "Step 1750: Accuracy = 0.8096\n",
      "New best accuracy: 0.8096 at step 1750\n",
      "已手动保存最佳模型到 ./results/mamba/full_scratch/best_model\n",
      "Step 2000: Recent average loss = 0.3982\n",
      "Step 2000: Accuracy = 0.8062\n",
      "Step 2250: Accuracy = 0.8303\n",
      "New best accuracy: 0.8303 at step 2250\n",
      "已手动保存最佳模型到 ./results/mamba/full_scratch/best_model\n",
      "Step 2500: Recent average loss = 0.3502\n",
      "Step 2500: Accuracy = 0.7959\n",
      "Step 2750: Accuracy = 0.8177\n",
      "Step 3000: Recent average loss = 0.2393\n",
      "Step 3000: Accuracy = 0.8154\n",
      "Step 3250: Accuracy = 0.8222\n",
      "Step 3500: Recent average loss = 0.1786\n",
      "Step 3500: Accuracy = 0.8200\n",
      "Step 3750: Accuracy = 0.8028\n",
      "Step 4000: Recent average loss = 0.1437\n",
      "Step 4000: Accuracy = 0.8177\n",
      "Step 4250: Accuracy = 0.8222\n",
      "Step 4500: Recent average loss = 0.1115\n",
      "Step 4500: Accuracy = 0.7959\n",
      "Step 4750: Accuracy = 0.8188\n",
      "Step 5000: Recent average loss = 0.0908\n",
      "Step 5000: Accuracy = 0.8131\n",
      "Step 5250: Accuracy = 0.8165\n",
      "Step 5500: Recent average loss = 0.0702\n",
      "Step 5500: Accuracy = 0.8188\n",
      "Step 5750: Accuracy = 0.8005\n",
      "Step 6000: Recent average loss = 0.0551\n",
      "Step 6000: Accuracy = 0.8188\n",
      "Step 6250: Accuracy = 0.8073\n",
      "Training completed. Best accuracy: 0.8303 at step 2250\n",
      "加载最佳模型进行最终评估...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 6312: Accuracy = 0.8303\n",
      "Testing inference speed...\n",
      "Testing long sequence performance...\n",
      "Testing sequence length: 128\n",
      "Testing sequence length: 256\n",
      "Testing sequence length: 512\n",
      "Testing sequence length: 1024\n",
      "Testing sequence length: 2048\n",
      "Testing sequence length: 4096\n",
      "Testing sequence length: 8192\n",
      "Testing sequence length: 16384\n",
      "Mamba training and evaluation completed successfully!\n",
      "\n",
      "[2/3] Training and evaluating Transformer model...\n",
      "\n",
      "==================================================\n",
      "Running Transformer experiment  (FROM SCRATCH)\n",
      "==================================================\n",
      "使用本地缓存的tokenizer\n",
      "从头创建优化的BERT模型...\n",
      "Transformer model parameters: 66,956,546\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6312' max='6312' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6312/6312 32:11, Epoch 5/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Memory Usage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.950600</td>\n",
       "      <td>0.702350</td>\n",
       "      <td>0.489679</td>\n",
       "      <td>2294.445312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.838200</td>\n",
       "      <td>0.707287</td>\n",
       "      <td>0.509174</td>\n",
       "      <td>2296.507812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.817900</td>\n",
       "      <td>0.733286</td>\n",
       "      <td>0.509174</td>\n",
       "      <td>2296.507812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.796500</td>\n",
       "      <td>0.702022</td>\n",
       "      <td>0.509174</td>\n",
       "      <td>2296.562500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.770200</td>\n",
       "      <td>0.768985</td>\n",
       "      <td>0.544725</td>\n",
       "      <td>2296.660156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.741800</td>\n",
       "      <td>0.614473</td>\n",
       "      <td>0.692661</td>\n",
       "      <td>2296.667969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.644300</td>\n",
       "      <td>0.606957</td>\n",
       "      <td>0.748853</td>\n",
       "      <td>2296.667969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.542200</td>\n",
       "      <td>0.597611</td>\n",
       "      <td>0.754587</td>\n",
       "      <td>2296.667969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.447000</td>\n",
       "      <td>0.557777</td>\n",
       "      <td>0.776376</td>\n",
       "      <td>2296.667969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.396200</td>\n",
       "      <td>0.647779</td>\n",
       "      <td>0.782110</td>\n",
       "      <td>2296.667969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.381300</td>\n",
       "      <td>0.624351</td>\n",
       "      <td>0.782110</td>\n",
       "      <td>2296.679688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.362900</td>\n",
       "      <td>0.539136</td>\n",
       "      <td>0.780963</td>\n",
       "      <td>2296.679688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>0.334900</td>\n",
       "      <td>0.586820</td>\n",
       "      <td>0.790138</td>\n",
       "      <td>2296.687500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.303900</td>\n",
       "      <td>0.679916</td>\n",
       "      <td>0.792431</td>\n",
       "      <td>2296.699219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>0.292200</td>\n",
       "      <td>0.605347</td>\n",
       "      <td>0.794725</td>\n",
       "      <td>2307.953125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.277200</td>\n",
       "      <td>0.548205</td>\n",
       "      <td>0.814220</td>\n",
       "      <td>2296.718750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4250</td>\n",
       "      <td>0.273100</td>\n",
       "      <td>0.602067</td>\n",
       "      <td>0.814220</td>\n",
       "      <td>2296.718750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.246300</td>\n",
       "      <td>0.737583</td>\n",
       "      <td>0.780963</td>\n",
       "      <td>2296.718750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4750</td>\n",
       "      <td>0.251800</td>\n",
       "      <td>0.726553</td>\n",
       "      <td>0.795872</td>\n",
       "      <td>2296.718750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.252300</td>\n",
       "      <td>0.598936</td>\n",
       "      <td>0.805046</td>\n",
       "      <td>2296.718750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5250</td>\n",
       "      <td>0.244800</td>\n",
       "      <td>0.613414</td>\n",
       "      <td>0.798165</td>\n",
       "      <td>2296.734375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.220500</td>\n",
       "      <td>0.638149</td>\n",
       "      <td>0.807339</td>\n",
       "      <td>2296.734375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5750</td>\n",
       "      <td>0.216200</td>\n",
       "      <td>0.646564</td>\n",
       "      <td>0.800459</td>\n",
       "      <td>2296.734375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.230300</td>\n",
       "      <td>0.591656</td>\n",
       "      <td>0.810780</td>\n",
       "      <td>2296.734375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6250</td>\n",
       "      <td>0.214800</td>\n",
       "      <td>0.649484</td>\n",
       "      <td>0.810780</td>\n",
       "      <td>2296.742188</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 250: Accuracy = 0.4897\n",
      "New best accuracy: 0.4897 at step 250\n",
      "已手动保存最佳模型到 ./results/transformer/full_scratch/best_model\n",
      "Step 500: Recent average loss = 0.8944\n",
      "Step 500: Accuracy = 0.5092\n",
      "New best accuracy: 0.5092 at step 500\n",
      "已手动保存最佳模型到 ./results/transformer/full_scratch/best_model\n",
      "Step 750: Accuracy = 0.5092\n",
      "Step 1000: Recent average loss = 0.8508\n",
      "Step 1000: Accuracy = 0.5092\n",
      "Step 1250: Accuracy = 0.5447\n",
      "New best accuracy: 0.5447 at step 1250\n",
      "已手动保存最佳模型到 ./results/transformer/full_scratch/best_model\n",
      "Step 1500: Recent average loss = 0.8192\n",
      "Step 1500: Accuracy = 0.6927\n",
      "New best accuracy: 0.6927 at step 1500\n",
      "已手动保存最佳模型到 ./results/transformer/full_scratch/best_model\n",
      "Step 1750: Accuracy = 0.7489\n",
      "New best accuracy: 0.7489 at step 1750\n",
      "已手动保存最佳模型到 ./results/transformer/full_scratch/best_model\n",
      "Step 2000: Recent average loss = 0.7627\n",
      "Step 2000: Accuracy = 0.7546\n",
      "New best accuracy: 0.7546 at step 2000\n",
      "已手动保存最佳模型到 ./results/transformer/full_scratch/best_model\n",
      "Step 2250: Accuracy = 0.7764\n",
      "New best accuracy: 0.7764 at step 2250\n",
      "已手动保存最佳模型到 ./results/transformer/full_scratch/best_model\n",
      "Step 2500: Recent average loss = 0.6945\n",
      "Step 2500: Accuracy = 0.7821\n",
      "New best accuracy: 0.7821 at step 2500\n",
      "已手动保存最佳模型到 ./results/transformer/full_scratch/best_model\n",
      "Step 2750: Accuracy = 0.7821\n",
      "Step 3000: Recent average loss = 0.5900\n",
      "Step 3000: Accuracy = 0.7810\n",
      "Step 3250: Accuracy = 0.7901\n",
      "New best accuracy: 0.7901 at step 3250\n",
      "已手动保存最佳模型到 ./results/transformer/full_scratch/best_model\n",
      "Step 3500: Recent average loss = 0.4925\n",
      "Step 3500: Accuracy = 0.7924\n",
      "New best accuracy: 0.7924 at step 3500\n",
      "已手动保存最佳模型到 ./results/transformer/full_scratch/best_model\n",
      "Step 3750: Accuracy = 0.7947\n",
      "New best accuracy: 0.7947 at step 3750\n",
      "已手动保存最佳模型到 ./results/transformer/full_scratch/best_model\n",
      "Step 4000: Recent average loss = 0.3982\n",
      "Step 4000: Accuracy = 0.8142\n",
      "New best accuracy: 0.8142 at step 4000\n",
      "已手动保存最佳模型到 ./results/transformer/full_scratch/best_model\n",
      "Step 4250: Accuracy = 0.8142\n",
      "Step 4500: Recent average loss = 0.3315\n",
      "Step 4500: Accuracy = 0.7810\n",
      "Step 4750: Accuracy = 0.7959\n",
      "Step 5000: Recent average loss = 0.2976\n",
      "Step 5000: Accuracy = 0.8050\n",
      "Step 5250: Accuracy = 0.7982\n",
      "Step 5500: Recent average loss = 0.2697\n",
      "Step 5500: Accuracy = 0.8073\n",
      "Step 5750: Accuracy = 0.8005\n",
      "Step 6000: Recent average loss = 0.2505\n",
      "Step 6000: Accuracy = 0.8108\n",
      "Step 6250: Accuracy = 0.8108\n",
      "Training completed. Best accuracy: 0.8142 at step 4000\n",
      "加载最佳模型进行最终评估...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 6312: Accuracy = 0.8142\n",
      "Testing inference speed...\n",
      "Testing long sequence performance...\n",
      "Testing sequence length: 128\n",
      "Testing sequence length: 256\n",
      "Testing sequence length: 512\n",
      "Testing sequence length: 1024\n",
      "Testing sequence length: 2048\n",
      "Testing sequence length: 4096\n",
      "Testing sequence length: 8192\n",
      "Testing sequence length: 16384\n",
      "Transformer training and evaluation completed successfully!\n",
      "\n",
      "[3/3] Comparing model results...\n",
      "\n",
      "==================================================\n",
      "DETAILED MODEL COMPARISON\n",
      "==================================================\n",
      "\n",
      "CLASSIFICATION PERFORMANCE\n",
      "------------------------------\n",
      "Accuracy:\n",
      "  Mamba: 0.8303\n",
      "  Transformer: 0.8142\n",
      "  Difference: 0.0161\n",
      "  Ratio (M/T): 1.0197\n",
      "\n",
      "Precision:\n",
      "  Mamba: 0.8318\n",
      "  Transformer: 0.8190\n",
      "  Difference: 0.0128\n",
      "  Ratio (M/T): 1.0157\n",
      "\n",
      "Recall:\n",
      "  Mamba: 0.8356\n",
      "  Transformer: 0.8153\n",
      "  Difference: 0.0203\n",
      "  Ratio (M/T): 1.0249\n",
      "\n",
      "F1 Score:\n",
      "  Mamba: 0.8337\n",
      "  Transformer: 0.8172\n",
      "  Difference: 0.0166\n",
      "  Ratio (M/T): 1.0203\n",
      "\n",
      "ROC AUC:\n",
      "  Mamba: 0.9025\n",
      "  Transformer: 0.8878\n",
      "  Difference: 0.0147\n",
      "  Ratio (M/T): 1.0166\n",
      "\n",
      "\n",
      "EFFICIENCY METRICS\n",
      "------------------------------\n",
      "Training Time (s):\n",
      "  Mamba: 2039.6242\n",
      "  Transformer: 1935.5878\n",
      "  Ratio (T/M): 0.9490\n",
      "  Transformer is more efficient\n",
      "\n",
      "Inference Speed (s/sample):\n",
      "  Mamba: 0.0066\n",
      "  Transformer: 0.0049\n",
      "  Ratio (T/M): 0.7423\n",
      "  Transformer is more efficient\n",
      "\n",
      "Memory Usage (MB):\n",
      "  Mamba: 2163.0391\n",
      "  Transformer: 2297.4883\n",
      "  Ratio (T/M): 1.0622\n",
      "  Mamba is more efficient\n",
      "\n",
      "\n",
      "LONG SEQUENCE PERFORMANCE\n",
      "------------------------------\n",
      "Sequence Length 128:\n",
      "  Time (s): Mamba = 0.00666, Transformer = 0.00478, Ratio (T/M) = 0.72x\n",
      "  Memory (MB): Mamba = 2629.04, Transformer = 7866.57, Ratio (T/M) = 2.99x\n",
      "  Transformer is faster\n",
      "  Mamba is more memory efficient\n",
      "\n",
      "Sequence Length 256:\n",
      "  Time (s): Mamba = 0.00674, Transformer = 0.00497, Ratio (T/M) = 0.74x\n",
      "  Memory (MB): Mamba = 2629.04, Transformer = 7866.57, Ratio (T/M) = 2.99x\n",
      "  Transformer is faster\n",
      "  Mamba is more memory efficient\n",
      "\n",
      "Sequence Length 512:\n",
      "  Time (s): Mamba = 0.00678, Transformer = 0.00679, Ratio (T/M) = 1.00x\n",
      "  Memory (MB): Mamba = 2629.04, Transformer = 7866.57, Ratio (T/M) = 2.99x\n",
      "  Mamba is faster\n",
      "  Mamba is more memory efficient\n",
      "\n",
      "Sequence Length 1024:\n",
      "  Time (s): Mamba = 0.01246, Transformer = 0.01640, Ratio (T/M) = 1.32x\n",
      "  Memory (MB): Mamba = 2629.04, Transformer = 7866.57, Ratio (T/M) = 2.99x\n",
      "  Mamba is faster\n",
      "  Mamba is more memory efficient\n",
      "\n",
      "Sequence Length 2048:\n",
      "  Time (s): Mamba = 0.02406, Transformer = 0.03846, Ratio (T/M) = 1.60x\n",
      "  Memory (MB): Mamba = 2629.04, Transformer = 7866.57, Ratio (T/M) = 2.99x\n",
      "  Mamba is faster\n",
      "  Mamba is more memory efficient\n",
      "\n",
      "Sequence Length 4096:\n",
      "  Time (s): Mamba = 0.04616, Transformer = 0.10968, Ratio (T/M) = 2.38x\n",
      "  Memory (MB): Mamba = 2758.79, Transformer = 7866.57, Ratio (T/M) = 2.85x\n",
      "  Mamba is faster\n",
      "  Mamba is more memory efficient\n",
      "\n",
      "Sequence Length 8192:\n",
      "  Time (s): Mamba = 0.10009, Transformer = 0.35546, Ratio (T/M) = 3.55x\n",
      "  Memory (MB): Mamba = 4461.39, Transformer = 7866.57, Ratio (T/M) = 1.76x\n",
      "  Mamba is faster\n",
      "  Mamba is more memory efficient\n",
      "\n",
      "Sequence Length 16384:\n",
      "  Time (s): Mamba = 0.19732, Transformer = 1.30099, Ratio (T/M) = 6.59x\n",
      "  Memory (MB): Mamba = 7866.57, Transformer = 7866.57, Ratio (T/M) = 1.00x\n",
      "  Mamba is faster\n",
      "  Transformer is more memory efficient\n",
      "\n",
      "\n",
      "Detailed comparison completed. Results saved to 'results/comparison/' directory.\n",
      "Model comparison completed successfully!\n",
      "\n",
      "============================== EXPERIMENT COMPLETED ==============================\n",
      "Total experiment time: 1h 7m 18.80s\n"
     ]
    }
   ],
   "source": [
    "# 用法示例 (取消注释以运行)\n",
    "#experiment_results = run_complete_experiment(test_mode=True)  # 测试模式\n",
    "experiment_results = run_complete_experiment(test_mode=False)  # 完整模式"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
